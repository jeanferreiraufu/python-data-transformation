# -*- coding: utf-8 -*-
"""UNET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnuEluiGglV0FKTroooagpi_IKnZIkGB

DataUtils
"""

##!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

"""

import numpy as np
from torch.utils.data import Dataset
import json
import cv2
import torch
import os.path as osp
import glob

class SegmentationDataset(Dataset):
    """Segmentation dataset loader."""

    def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution = (800, 450), augmentation = False, transform=None):
    #def __init__(self, json_folder, img_folder, is_train, class_to_id, resolution = (1280, 960), augmentation = False, transform=None):
        """
        Args:
            json_folder (str): Path to folder that contains the annotations.
            img_folder (str): Path to all images.
            is_train (bool): Is this a training dataset ?
            augmentation (bool): Do dataset augmentation (crete artificial variance) ?
        """

        self.gt_file_list = glob.glob(osp.join(json_folder, '*.json'))

        self.total_samples = len(self.gt_file_list)
        self.img_folder = img_folder
        self.is_train = is_train
        self.transform = transform
        self.augmentation = augmentation
        self.resolution = resolution
        self.class_to_id = class_to_id


        # Mean and std are needed because we start from a pre trained net
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):

        gt_file = self.gt_file_list[idx]
        img_number_str = gt_file.split('.')[0].split('/')[-1]
	# Abre Json
        gt_json = json.load(open(gt_file, 'r'))
	# Abre imagem
        img_np = cv2.imread(osp.join(self.img_folder, img_number_str + '.JPG'), cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)
        original_shape = img_np.shape
        img_np = cv2.resize(img_np, (self.resolution[0], self.resolution[1]))[..., ::-1]
        img_np = np.ascontiguousarray(img_np)
	# Cria imagem zerada
        label_np = np.zeros((img_np.shape[0], img_np.shape[1]))
        label_np[...] = -1

	# Para todos poligonos
        for shape in gt_json['shapes']:
            # Transforma os pontos do poligono em array
            points_np = np.array(shape['points'], dtype = np.float64)

	    # Ajusta os pontos porque eu mudo o resolucao (pode ignorar)
            points_np[:, 0] *= self.resolution[0]/original_shape[1]
            points_np[:, 1] *= self.resolution[1]/original_shape[0]
	    # As coordenadas dos pontos que formam o poligono tem que ser inteiros
            points_np = np.round(points_np).astype(np.int64)
	    # Coloca os pontos no formato certo para o opencv
            points_np = points_np.reshape((-1,1,2))
	    # Pinta o poligono usando o opencv com o valor referente ao label
            label_np = cv2.fillPoly(label_np, [points_np], self.class_to_id[shape['label']])

        # Transforma o GT em inteiro
        label_np = label_np.astype(np.int32)

        # Data augmentation?
        if self.is_train and self.augmentation:
            if np.random.rand() > 0.5:
                img_np = np.fliplr(img_np)
                label_np = np.fliplr(label_np)
                img_np = np.ascontiguousarray(img_np)
                label_np = np.ascontiguousarray(label_np)

        img_pt = img_np.astype(np.float32) / 255.0
        for i in range(3):
            img_pt[..., i] -= self.mean[i]
            img_pt[..., i] /= self.std[i]

        img_pt = img_pt.transpose(2,0,1)

        img_pt = torch.from_numpy(img_pt)
        label_pt = torch.from_numpy(label_np).long()
        #print(img_number_str, img_pt.shape)

        sample = {'image': img_pt, 'gt': label_pt, 'image_original': img_np}

        if self.transform:
            sample = self.transform(sample)

        return sample

"""VGG UNet"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

"""
import torch
import torchvision
import numpy as np

class UNetVgg(torch.nn.Module):
    """
    BorderNetwork is a NN that aims to detected border and classify occlusion.
    The architecture is a VGG without the last pool layer. After that we
    have two paths, one for regression and one for classification (occlusion).
    """

    def __init__(self, nClasses):
        super(UNetVgg, self).__init__()

        vgg16pre = torchvision.models.vgg16(pretrained=True)
        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])
        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])
        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])
        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])
        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])


        self.smooth0 = torch.nn.Sequential(
                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True),
                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True)
                )
        self.smooth1 = torch.nn.Sequential(
                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True),
                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True)
                )
        self.smooth2 = torch.nn.Sequential(
                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True),
                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True)
                )
        self.smooth3 = torch.nn.Sequential(
                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True),
                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),
                torch.nn.ReLU(True)
                )


        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        """
        Args:
            x (torch.tensor): A tensor of size (batch, 3, H, W)
        Returns:
            reg_out (torch.tensor): A tensor with results of the regression (batch, 4).
            cls_out (torch.tensor): A tensor with results of the classification (batch, 2).
        """

        feat0 = self.vgg0(x)
        feat1 = self.vgg1(feat0)
        feat2 = self.vgg2(feat1)
        feat3 = self.vgg3(feat2)
        feat4 = self.vgg4(feat3)

        _,_,H,W = feat3.size()
        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)
        concat3 = torch.cat([feat3, up3], 1)
        end3 = self.smooth3(concat3)

        _,_,H,W = feat2.size()
        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)
        concat2 = torch.cat([feat2, up2], 1)
        end2 = self.smooth2(concat2)

        _,_,H,W = feat1.size()
        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)
        concat1 = torch.cat([feat1, up1], 1)
        end1 = self.smooth1(concat1)

        _,_,H,W = feat0.size()
        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)
        concat0 = torch.cat([feat0, up0], 1)
        end0 = self.smooth0(concat0)

        return self.final(end0)


    @staticmethod
    def eval_net_with_loss(model, inp, gt, class_weights, device):
        """
        Evaluate network including loss.

        Args:
            model (torch.nn.Module): The model.
            inp (torch.tensor): A tensor (float32) of size (batch, 3, H, W)
            gt (torch.tensor): A tensor (long) of size (batch, 1, H, W) with the groud truth (0 to num_classes-1).
            class_weights (list of float): A list with len == num_classes.
            device (torch.device): device to perform computation

        Returns:
            out (torch.tensor): Network output.
            loss (torch.tensor): Tensor with the total loss.

        """
        weights = torch.from_numpy(np.array(class_weights, dtype=np.float32)).to(device)
        out = model(inp)

        softmax = torch.nn.functional.log_softmax(out, dim = 1)
        loss = torch.nn.functional.nll_loss(softmax, gt, ignore_index=-1, weight=weights)

        return (out, loss)

    @staticmethod
    def get_params_by_kind(model, n_base = 7):

        base_vgg_bias = []
        base_vgg_weight = []
        core_weight = []
        core_bias = []

        for name, param in model.named_parameters():
            if 'vgg' in name and ('weight' in name or 'bias' in name):
                vgglayer = int(name.split('.')[-2])

                if vgglayer <= n_base:
                    if 'bias' in name:
                        print('Adding %s to base vgg bias.' % (name))
                        base_vgg_bias.append(param)
                    else:
                        base_vgg_weight.append(param)
                        print('Adding %s to base vgg weight.' % (name))
                else:
                    if 'bias' in name:
                        print('Adding %s to core bias.' % (name))
                        core_bias.append(param)
                    else:
                        print('Adding %s to core weight.' % (name))
                        core_weight.append(param)

            elif ('weight' in name or 'bias' in name):
                if 'bias' in name:
                    print('Adding %s to core bias.' % (name))
                    core_bias.append(param)
                else:
                    print('Adding %s to core weight.' % (name))
                    core_weight.append(param)

        return (base_vgg_weight, base_vgg_bias, core_weight, core_bias)

# End class

"""pspnet

"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Add, Dense,Lambda, Activation,Concatenate,Reshape,Dropout, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dot, Conv2DTranspose,Cropping2D

def convolutional_block(X, filters,s = 2,rate=1):
    F1, F2, F3 = filters
    X_shortcut = X
    X = Conv2D(F1, (1, 1), strides = (s, s))(X)
    X = BatchNormalization(axis = 3,momentum = 0.95)(X)
    X = Activation('relu')(X)
    X = ZeroPadding2D((rate, rate))(X)
    X = Conv2D(F2, (3, 3), strides = (1, 1),dilation_rate=rate)(X)
    X = BatchNormalization(axis = 3,momentum = 0.95)(X)
    X = Activation('relu')(X)
    X = Conv2D(F3, (1, 1), strides = (1, 1))(X)
    X = BatchNormalization(axis = 3,momentum = 0.95)(X)
    X_proj = Conv2D(F3, (1, 1), strides = (s, s))(X_shortcut)
    X_proj = BatchNormalization(axis = 3,momentum = 0.95)(X_proj)
    X = Add()([X,X_proj])
    X = Activation('relu')(X)
    return X

def identity_block(X, filters):

    F1, F2, F3 = filters

    X_shortcut = X

    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)
    X = BatchNormalization(axis = 3)(X)
    X = Activation('relu')(X)

    X = Conv2D(filters = F2, kernel_size = (3, 3), strides = (1,1), padding = 'same')(X)
    X = BatchNormalization(axis=3)(X)
    X = Activation('relu')(X)

    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)
    X = BatchNormalization(axis=3)(X)

    X = layers.add([X, X_shortcut])
    X = Activation('relu')(X)

    return X


def global_average_pooling(input, gap_size):
    w, h, c = (input.shape[1:])
    x = AveragePooling2D((w/gap_size, h/gap_size))(input)
    x = Conv2D(c//4, (1,1), padding="same", activation='relu')(x)
    x = BatchNormalization()(x)
    x = tf.image.resize(x, (w, h))
    return x

def resnet(input):
  X = ZeroPadding2D((1, 1))(input)
  X = Conv2D(64, (3, 3), strides = (2, 2))(X)
  X = BatchNormalization(axis = 3,momentum = 0.95)(X)
  X = Activation('relu')(X)
  X = ZeroPadding2D((1, 1))(X)
  X = Conv2D(64, (3, 3), strides = (1, 1))(X)
  X = BatchNormalization(axis = 3,momentum = 0.95)(X)
  X = Activation('relu')(X)
  X = ZeroPadding2D((1, 1))(X)
  X = Conv2D(128, (3, 3), strides = (1, 1))(X)
  X = BatchNormalization(axis = 3,momentum = 0.95)(X)
  X = Activation('relu')(X)
  X = ZeroPadding2D((1, 1))(X)
  X = MaxPooling2D((3, 3), strides=(2, 2))(X)

  X = convolutional_block(X, [64,64,256],s = 1)
  X = identity_block(X,[64,64,256])
  X = identity_block(X,[64,64,256])

  X = convolutional_block(X, [128,128,512],s = 2)
  X = identity_block(X,[128,128,512])
  X = identity_block(X,[128,128,512])
  X = identity_block(X,[128,128,512])

  X = convolutional_block(X, [256,256,1024],s = 1,rate=2)
  X = identity_block(X,[256,256,1024])
  X = identity_block(X,[256,256,1024])
  X = identity_block(X,[256,256,1024])
  X = identity_block(X,[256,256,1024])
  X = identity_block(X,[256,256,1024])

  X = convolutional_block(X, [512,512,2048],s = 1,rate=4)
  X = identity_block(X,[512,512,2048])
  X = identity_block(X,[512,512,2048])
  return X

def pspnet(input_shape, n_classes):
  input =  tf.keras.layers.Input(shape=input_shape)
  feature_map = resnet(input)
  pooling_1 = global_average_pooling(feature_map, 1)
  pooling_2 = global_average_pooling(feature_map, 2)
  pooling_3 = global_average_pooling(feature_map, 3)
  pooling_4 = global_average_pooling(feature_map, 6)
  x = tf.keras.layers.Concatenate(axis=-1)([pooling_1,pooling_2,pooling_3,pooling_4])

  x = Conv2D(512, (3,3), padding="same")(x)
  x = BatchNormalization()(x)
  x = Activation('relu')(x)
  x = Conv2D(n_classes, (1,1), padding="same", activation='relu')(x)
  x = tf.image.resize(x, (input_shape[0], input_shape[1]))

  return tf.keras.Model(input , outputs = x)

"""pspunetvvg16"""

import numpy as np
from tensorflow.keras import Sequential
import tensorflow as tf
from tensorflow.keras.layers import  Flatten, Conv2D, MaxPooling2D, Dense, Dropout, Softmax, Conv2DTranspose, BatchNormalization,Activation, AveragePooling2D


def vgg16(input_shape):
    model = Sequential()
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu", input_shape=input_shape))
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu", name = "copy_crop1"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu", name = "copy_crop2"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu", name = "copy_crop3"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block3_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu", name = "copy_crop4"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block4_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu", name = "last_layer"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    return model

def global_average_pooling(input, gap_size):
    w, h, c = (input.shape[1:])
    x = AveragePooling2D((w/gap_size, h/gap_size))(input)
    x = Conv2D(c//4, (1,1), padding="same", activation='relu')(x)
    x = BatchNormalization()(x)
    x = tf.image.resize(x, (w, h))
    return x

def pspunet_vgg16(input_shape, n_classes):
    vgg_model = vgg16(input_shape)
    vgg_model.load_weights("/content/drive/MyDrive/Diretorio_colab/vgg16.h5")
    vgg_model.trainable = False

    layer_names  = ["copy_crop1", "copy_crop2",  "copy_crop3" ,"copy_crop4"]
    layers = [vgg_model.get_layer(name).output for name in layer_names]

    extract_model = tf.keras.Model(inputs=vgg_model.input, outputs=layers)
    input= tf.keras.layers.Input(shape =input_shape)
    output_layers = extract_model(inputs = input)
    last_layer = output_layers[-1]

    feature_map = last_layer
    pooling_1 = global_average_pooling(feature_map, 1)
    pooling_2 = global_average_pooling(feature_map, 2)
    pooling_3 = global_average_pooling(feature_map, 3)
    pooling_4 = global_average_pooling(feature_map, 6)
    x = tf.keras.layers.Concatenate(axis=-1)([pooling_1,pooling_2,pooling_3,pooling_4])
    x =  Conv2D(256, (1,1), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    #x = Conv2DTranspose(512, 4, (2,2), padding = "same", activation = "relu")(last_layer)
    x = tf.keras.layers.Concatenate()([x, output_layers[3]])

    x =  Conv2D(256, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(256, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2DTranspose(256, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[2]])

    x =  Conv2D(128, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x =  Conv2D(128, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2DTranspose(128, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[1]])


    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)


    x = Conv2DTranspose(64, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[0]])

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(n_classes, (1,1), activation = "relu")(x)

    return tf.keras.Model(inputs = input , outputs = x)

"""fcn"""

!pip install -q git+https://github.com/tensorflow/examples.git

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.applications.vgg16 import VGG16
import tensorflow as tf
from tensorflow.keras.layers import  Flatten, Conv2D, MaxPooling2D, Dense, Dropout, Softmax, Conv2DTranspose, BatchNormalization
#from tensorflow_examples.models.pix2pix import data_download
from tensorflow_examples.models.pix2pix import pix2pix


#pasta = '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu'

def vgg16(input_shape):
    model = Sequential()
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu", input_shape=input_shape))
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block3_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block4_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block5_pool'))
    return model


def fcn(input_shape, n_class):
    vgg_model = vgg16(input_shape)
    vgg_model.load_weights("/content/drive/MyDrive/vgg16.h5")
    #vgg_model = VGG16(input_shape = input_shape, weights = 'imagenet', include_top=False)
    vgg_model.trainable = False
    layer_names  = ['block3_pool', 'block4_pool', 'block5_pool']
    layers = [vgg_model.get_layer(name).output for name in layer_names]
    fcn_model = tf.keras.Model(inputs=vgg_model.input, outputs=layers)
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = inputs
    skip = fcn_model(x)
    vgg_last_layer = skip[-1]
    x = vgg_last_layer
    x =  Conv2D(4096, (7,7), padding = "same", activation = "relu")(x)
    x = Conv2D(3, (1,1), padding = "same", activation = "relu")(x)
    x = Conv2DTranspose(512, 4, (2,2), padding = "same", activation = "relu")(x)
    x = BatchNormalization()(x)
    x = tf.keras.layers.Concatenate()([skip[1], x])
    x = Conv2DTranspose(256, 4, (2,2), padding = "same", activation = "relu")(x)
    x = BatchNormalization()(x)
    x = tf.keras.layers.Concatenate()([skip[0], x])
    x = BatchNormalization()(x)
    x = Conv2DTranspose(n_class, 16, (8,8), padding = "same", activation = "relu")(x)
    model =  tf.keras.Model(inputs = inputs , outputs = x)
    return model

"""psp_unet"""

import numpy as np
from tensorflow.keras import Sequential
import tensorflow as tf
from tensorflow.keras.layers import  Flatten, Conv2D, MaxPooling2D, Dense, Dropout, Softmax, Conv2DTranspose, BatchNormalization,Activation, AveragePooling2D


def vgg16(input_shape):
    model = Sequential()
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu", input_shape=input_shape))
    model.add(Conv2D(64, (3,3), padding ="same", activation = "relu", name = "copy_crop1"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(128, (3,3), padding = "same", activation = "relu", name = "copy_crop2"))
    model.add(MaxPooling2D((2,2), strides=(2, 2)))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(256, (3,3), padding = "same", activation = "relu", name = "copy_crop3"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block3_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu", name = "copy_crop4"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = 'block4_pool'))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(Conv2D(512, (3,3), padding = "same", activation = "relu"))
    model.add(MaxPooling2D((2,2), strides=(2, 2), name = "last_layer"))
    return model

def global_average_pooling(input, gap_size):
    w, h, c = (input.shape[1:])
    x = AveragePooling2D((w/gap_size, h/gap_size))(input)
    x = Conv2D(c//4, (1,1), padding="same", activation='relu')(x)
    x = BatchNormalization()(x)
    x = tf.image.resize(x, (w, h))
    return x


def contract_path(input_shape):
    input= tf.keras.layers.Input(shape = input_shape)
    x =  Conv2D(64, (3,3), padding = "same", activation = "relu")(input)
    x =  Conv2D(64, (3,3), padding = "same", activation = "relu", name = "copy_crop1")(x)
    x = MaxPooling2D((2, 2))(x)
    x =  Conv2D(128, (3,3), padding = "same", activation = "relu")(x)
    x =  Conv2D(128, (3,3), padding = "same", activation = "relu", name = "copy_crop2")(x)
    x = MaxPooling2D((2, 2))(x)
    x =  Conv2D(256, (3,3), padding = "same", activation = "relu")(x)
    x =  Conv2D(256, (3,3), padding = "same", activation = "relu", name = "copy_crop3")(x)
    x = MaxPooling2D((2, 2))(x)
    x =  Conv2D(512, (3,3), padding = "same", activation = "relu")(x)
    x =  Conv2D(512, (3,3), padding = "same", activation = "relu", name = "copy_crop4")(x)
    x = MaxPooling2D((2, 2))(x)
    x =  Conv2D(1024, (3,3), padding = "same", activation = "relu")(x)
    x =  Conv2D(1024, (3,3), padding = "same", activation = "relu", name = "last_layer")(x)
    contract_path =  tf.keras.Model(inputs = input, outputs = x)
    return contract_path

def pspunet(input_shape, n_classes):
    contract_model = contract_path(input_shape=input_shape)
    layer_names  = ["copy_crop1", "copy_crop2",  "copy_crop3" ,"copy_crop4"]
    layers = [contract_model.get_layer(name).output for name in layer_names]

    extract_model = tf.keras.Model(inputs=contract_model.input, outputs=layers)
    input= tf.keras.layers.Input(shape =input_shape)
    output_layers = extract_model(inputs = input)
    last_layer = output_layers[-1]

    feature_map = last_layer
    pooling_1 = global_average_pooling(feature_map, 1)
    pooling_2 = global_average_pooling(feature_map, 2)
    pooling_3 = global_average_pooling(feature_map, 3)
    pooling_4 = global_average_pooling(feature_map, 6)
    x = tf.keras.layers.Concatenate(axis=-1)([pooling_1,pooling_2,pooling_3,pooling_4])
    x =  Conv2D(256, (1,1), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = tf.keras.layers.Concatenate()([x, output_layers[3]])

    x =  Conv2D(256, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(256, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2DTranspose(256, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[2]])

    x =  Conv2D(128, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(128, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Conv2DTranspose(128, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[1]])


    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)


    x = Conv2DTranspose(64, 4, (2,2), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = tf.keras.layers.Concatenate()([x, output_layers[0]])

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(64, (3,3), padding = "same")(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x =  Conv2D(n_classes, (1,1), activation = "relu")(x)

    return tf.keras.Model(inputs = input , outputs = x)

"""Treinamento"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""test git"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf

label_rgb =dict()
label_rgb["background"] = np.array([0, 0, 0])
label_rgb["sidewalk_blocks"] = np.array([0, 0, 255])
label_rgb["sidewalk_cement"] = np.array([217, 217, 217])
label_rgb["sidewalk_urethane"] = np.array([198, 89, 17])
label_rgb["sidewalk_asphalt"] = np.array([128, 128, 128])
label_rgb["sidewalk_soil_stone"] = np.array([255, 230, 153])
label_rgb["sidewalk_damaged"] = np.array([55, 86, 35])
label_rgb["sidewalk_other"] = np.array([110, 168, 70])
label_rgb["braille_guide_blocks_normal"] = np.array([255, 255, 0])
label_rgb["braille_guide_blocks_damaged"] = np.array([128, 96, 0])
label_rgb["roadway_normal"] = np.array([255, 128, 255])
label_rgb["roadway_crosswalk"] =np.array([255, 0, 255])
label_rgb["alley_normal"] = np.array([230, 170, 255])
label_rgb["alley_crosswalk"] = np.array([208, 88, 255])
label_rgb["alley_speed_bump"] = np.array([138, 60, 200])
label_rgb["alley_damaged"] = np.array([88, 38, 128])
label_rgb["bike_lane_normal"] = np.array([255, 155, 155])
label_rgb[ "caution_zone_stairs"] = np.array([255, 192, 0])
label_rgb[ "caution_zone_manhole"] = np.array([255, 0, 0])
label_rgb[ "caution_zone_tree_zone"] = np.array([0, 255, 0])
label_rgb[ "caution_zone_grating"] = np.array([255, 128, 0])
label_rgb[ "caution_zone_repair_zone"] = np.array([105, 105, 255])

label_list = list()
label_list.append(["background"])
label_list.append(["bike_lane_normal", "sidewalk_asphalt", "sidewalk_urethane"])
label_list.append(["caution_zone_stairs", "caution_zone_manhole", "caution_zone_tree_zone", "caution_zone_grating", "caution_zone_repair_zone"])
label_list.append(["alley_crosswalk","roadway_crosswalk"])
label_list.append(["braille_guide_blocks_normal", "braille_guide_blocks_damaged"])
label_list.append(["roadway_normal","alley_normal","alley_speed_bump", "alley_damaged"])
label_list.append(["sidewalk_blocks","sidewalk_cement" , "sidewalk_soil_stone", "sidewalk_damaged","sidewalk_other"])


AUTOTUNE = tf.data.experimental.AUTOTUNE

IMG_WIDTH = 480
IMG_HEIGHT = 272

def train_images():
    top_foloder = "/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Train/"
    data_folder = os.listdir(top_foloder)
    data_folder.sort()
    data_folder = list(map(lambda x : top_foloder + x, data_folder))
    return data_folder

def mask_images():
    top_foloder = "/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Val/"
    mask_folder = os.listdir(top_foloder)
    mask_folder.sort()
    mask_folder = list(map(lambda x : top_foloder + x, mask_folder))
    return mask_folder

def get_label(file_path):
    img = tf.io.read_file(file_path)
    label_img = tf.image.decode_jpeg(img, channels=3)
    label_img = tf.image.resize(label_img, [IMG_HEIGHT, IMG_WIDTH])
    return label_img

def decode_img(img):
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
    return img

def process_path(file_path):
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img

def data_load():
    list_ds_train = tf.data.Dataset.list_files("/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Train/*", shuffle=False)
    list_ds_train_label = tf.data.Dataset.list_files("/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Val/*", shuffle=False)


    # Set `num_parallel__calls` so multiple images are loaded/processed in parallel.
    train_ds = list_ds_train.map(process_path, num_parallel_calls=AUTOTUNE)
    train_label_ds = list_ds_train_label.map(get_label, num_parallel_calls=AUTOTUNE)


    train_dataset = tf.data.Dataset.zip((train_ds, train_label_ds))


    list_ds_test = tf.data.Dataset.list_files("/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Train/*", shuffle=False)
    list_ds_test_label = tf.data.Dataset.list_files("/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Val/*", shuffle=False)


    # Set `num_parallel__calls` so multiple images are loaded/processed in parallel.
    test_ds = list_ds_test.map(process_path, num_parallel_calls=AUTOTUNE)
    test_label_ds = list_ds_test_label.map(get_label, num_parallel_calls=AUTOTUNE)


    test_dataset = tf.data.Dataset.zip((test_ds, test_label_ds))

    return train_dataset, test_dataset

def convert_class(label_img):
    for i in range(len(label_img)):
        for j in label_list[1]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 1
        for j in label_list[2]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 2
        for j in label_list[3]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 3
        for j in label_list[4]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 4
        for j in label_list[5]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 5
        for j in label_list[6]:
            label_img[i][(label_img[i]==label_rgb[j]).all(axis=2)] = 6

        label_img[i][(label_img[i]!=0 ) & (label_img[i]!=1 ) &(label_img[i]!= 2) &(label_img[i]!= 3) &(label_img[i]!= 4) &(label_img[i]!= 5) &(label_img[i]!= 6)]=0
    label_img = np.array(label_img)
    label_img= label_img[:,:,:,0]
    label_img = label_img[..., tf.newaxis]

    return label_img

from IPython.display import clear_output
#from data_loader import *
import matplotlib.pyplot as plt
import tensorflow as tf


def display(display_list):
    plt.figure(figsize=(7, 7))
    for i in range(3):
        plt.subplot(3, 3, i*3+1)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i][0]/255))
        plt.axis('off')
        plt.subplot(3, 3, i*3+2)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i][1]/255))
        plt.axis('off')
        plt.subplot(3, 3, i*3+3)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i][2]/255))
        plt.axis('off')
    plt.show()

def create_mask(pred_mask):
    pred_mask = tf.argmax(pred_mask, axis=-1)
    pred_mask = pred_mask[..., tf.newaxis]
    return pred_mask[0]

def show_predictions(image, label, model):

    if model:
        pred_mask = [model.predict(image[tf.newaxis, ...]) for image in image]
        display_list = [[image[i], label[i], create_mask(pred_mask[i])] for i in range(3)]
        display(display_list)
    else :
        display_list = [[image[i], label[i], image[i]] for i in range(3)]
        display(display_list)

#from data_loader import train_images, mask_images
import shutil

train_images = train_images()
mask_images = mask_images()

train_num = int(len(train_images)*0.75)

test_img = train_images[train_num:]
test_label = mask_images[train_num:]

test_img_path = ['/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Train/' +test_img[i].split("/")[-1] for i in range(len(test_img))]
test_label_path = ['/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Val/' +test_label[i].split("/")[-1] for i in range(len(test_label))]

for i in range(len(test_img)):
    shutil.move(test_img[i], test_img_path[i])

for i in range(len(test_label)):
    shutil.move(test_label[i], test_label_path[i])

import numpy as np
import tensorflow as tf
#import data_load
#import create_mask, show_predictions
from IPython.display import clear_output
#import pspunet
#import fcn
#import pspunet_vgg16

#conecta a pasta Google Drive
from google.colab import drive
drive.mount('/content/drive')

pasta = '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu'

#from model.unet import unet
import datetime
import time
from IPython.display import clear_output

IMG_WIDTH = 480
IMG_HEIGHT = 272
n_classes = 7

gpus = tf.config.experimental.list_physical_devices('GPU')

if gpus:
    try:
        tf.config.experimental.set_virtual_device_configuration(
       gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])
    except RuntimeError as e:
        print(e)


model = pspnet((IMG_HEIGHT, IMG_WIDTH,3), n_classes)

train_dataset, test_dataset = data_load()

optimizer = tf.keras.optimizers.Adam(1e-4)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

train_mIoU =tf.keras.metrics.MeanIoU(num_classes = n_classes, name = "train_mIoU")
test_mIoU =tf.keras.metrics.MeanIoU(num_classes = n_classes, name = "test_mIoU")

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = 'logs/fcn_log/gradient_tape/' + current_time + '/train'
test_log_dir = 'logs/fcn_log/gradient_tape/' + current_time + '/test'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)

@tf.function
def train_step(images, label):

    with tf.GradientTape() as tape:
        pred_img = model(images)
        loss =  loss_object(label, pred_img)

    gradients_of_model = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients_of_model, model.trainable_variables))

    train_loss(loss)
    train_accuracy(label, pred_img)

@tf.function
def test_step(images, label):
    pred_img = model(images)
    loss =  loss_object(label, pred_img)

    #pred_mask = tf.argmax(pred_img, axis=-1)
    #pred_mask = pred_mask[..., tf.newaxis]

    #test_mIoU(label, pred_mask)
    test_loss(loss)
    test_accuracy(label, pred_img)

def train(train_dataset, test_dataset, epochs, batch_size):

    for epoch in range(epochs):

        if epoch >=10:
            optimizer = tf.keras.optimizers.Adam(1e-5)


        start = time.time()

        train_loss.reset_states()
        train_accuracy.reset_states()

        test_loss.reset_states()
        test_accuracy.reset_states()

        count_img = 0
        batch_time = time.time()

        for image_batch, label_batch in train_dataset.batch(batch_size):
            count_img += batch_size
            label_batch = convert_class(label_batch.numpy())

            if tf.random.uniform(()) > 0.5:
                image_batch = tf.image.flip_left_right(image_batch)
                label_batch = tf.image.flip_left_right(label_batch)

            image_batch = tf.image.random_brightness(image_batch, 0.3)

            train_step(image_batch, label_batch)

            if count_img % 1000 == 0:
                clear_output(wait=True)
                show_predictions(image_batch[:3], label_batch[:3], model)
                print('epoch {}, step {}, train_acc {}, loss {} , time {}'.format(epoch+1,
                                                                        count_img,
                                                                        train_accuracy.result()*100,
                                                                        train_loss.result(),
                                                                        time.time()- batch_time))
                train_loss.reset_states()
                train_accuracy.reset_states()

                batch_time = time.time()


        count_img = 0
        batch_time = time.time()

        for image_batch, label_batch in test_dataset.batch(batch_size):
            count_img += batch_size
            label_batch = convert_class(label_batch.numpy())

            test_step(image_batch, label_batch)


            if count_img % 1000 == 0:
                clear_output(wait=True)
                show_predictions(image_batch[:3], label_batch[:3], model)
                print('epoch {}, step {}, test_acc {}, loss {} , time {}'.format(epoch+1,
                                                                        count_img,
                                                                        test_accuracy.result()*100,
                                                                        test_loss.result(),
                                                                        time.time()- batch_time))
                batch_time = time.time()



        clear_output(wait=True)

        for image_batch, label_batch in test_dataset.take(3).batch(3):
            label_batch = convert_class(label_batch.numpy())
            show_predictions(image_batch[:3], label_batch[:3], model)

        print ('Time for epoch {}  is {} sec'.format(epoch + 1, round(time.time()-start),3))

        print ('train_acc {}, loss {} , test_acc {}, loss {}'.format(train_accuracy.result()*100,
                                                                                 train_loss.result(),
                                                                                 test_accuracy.result()*100,
                                                                                 test_loss.result()
                                                                                 ))

        path = "model_h5/fcn/fcn_" + str(test_loss.result().numpy())+"_epoch_"+str(epoch+1)+".h5"
        model.save(path)

        with train_summary_writer.as_default():
                tf.summary.scalar('loss', train_loss.result(), step=epoch+1)
                tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch+1)
                #tf.summary.scalar('mIoU', train_mIoU.result(), step=epoch)

        with test_summary_writer.as_default():
                tf.summary.scalar('loss', test_loss.result(), step=epoch+1)
                tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch+1)
                #tf.summary.scalar('mIoU', test_mIoU.result(), step=epoch)

train(train_dataset, test_dataset, 20, 8)

"""test_jef"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

"""

import numpy as np
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
#import vgg_unet
import torch
#from data_utils import SegmentationDataset
from torch.utils.data import DataLoader
import time

#conecta a pasta Google Drive
from google.colab import drive
drive.mount('/content/drive')

pasta = '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu'
#pasta = '/content/drive/MyDrive/Bases Dados e Experimentos/unet-ufu/'

gt_folder_train = pasta + '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Train/'
gt_folder_val = pasta  + '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/Val/'
model_name = '/content/drive/MyDrive/Diretorio_colab/bck_unet/unet-ufu/segm2.pth'

#modo de processamento
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#critérios de parada
patience = 30
max_epochs = 300

#plots
plot_val = False
plot_train = False

#parametros imagem e classes
#Width x Height - MUST be divisible by 32
#resolution = (800, 448)
resolution = (1600, 896)
#resolution = (2400, 1344)
#resolution = (3200, 1792)
#resolution = (4000, 2240)

class_weights = [1, 1, 1]
nClasses = 3

#class_weights = [0.8, 1, 0.8, 0.8]
#nClasses = 4

#prepara imagens para treinamento
class_to_color = {'Solo': (127, 0, 0) , 'Saudavel': (0, 127, 127), 'Doenca': (0, 255, 0)}
#class_to_color = {'me': (127, 0, 0) , 'ma': (0, 127, 127), 'chao': (0, 255, 0), 'sdv': (127,127,0)}
class_to_id = {'Solo': 0, 'Saudavel': 1, 'Doenca': 2}
#class_to_id = {'me': 0, 'ma': 1, 'chao': 2, 'sdv': 3 }
id_to_class = {v: k for k, v in class_to_id.items()}

train_dataset = SegmentationDataset(gt_folder_train, gt_folder_train, True, class_to_id, resolution)#, True) #Data augmentation
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)

val_dataset = SegmentationDataset(gt_folder_val, gt_folder_val, False, class_to_id, resolution)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=False)


if plot_train:

    for i_batch, sample_batched in enumerate(train_loader):

            image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())
            gt = np.squeeze(sample_batched['gt'].cpu().numpy())

            color_label = np.zeros((resolution[1], resolution[0], 3))

            for key, val in id_to_class.items():
                color_label[gt == key] = class_to_color[val]

            plt.figure()
            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)
            plt.show()

            plt.figure()
            plt.imshow(color_label.astype(np.uint8))
            plt.show()


#model = vgg_unet.UNetVgg(nClasses).to(device)
#model = UNetVgg(nClasses).to(device)
model = pspnet(nClasses).to(device)

core_lr = 0.02
#base_vgg_weight, base_vgg_bias, core_weight, core_bias = vgg_unet.UNetVgg.get_params_by_kind(model, 7)
base_vgg_weight, base_vgg_bias, core_weight, core_bias = UNetVgg.get_params_by_kind(model, 7)


optimizer = torch.optim.SGD([{'params': base_vgg_bias, 'lr': 0.00001},    #'lr': 0.000001
                             {'params': base_vgg_weight, 'lr': 0.00001},  #'lr': 0.000001
                             {'params': core_bias, 'lr': core_lr},
                             {'params': core_weight, 'lr': core_lr, 'weight_decay': 0.0005}], momentum=0.9)
                             #'weight_decay': 0.0005
                             #momentum=0.9

# lr = lr * gamma a cada step_size epocas
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.2)


best_val_acc = -1
best_epoch = 0

# Start training...
start_time = time.time()

for epoch in range(max_epochs):

    print('Epoch %d starting...' % (epoch+1))

    lr_scheduler.step()

    model.train()

    mean_loss = 0.0

    n_correct = 0
    n_false = 0

    for i_batch, sample_batched in enumerate(train_loader):

        image = sample_batched['image'].to(device)
        gt = sample_batched['gt'].to(device)

        optimizer.zero_grad()
        output, total_loss = model.eval_net_with_loss(model, image, gt, class_weights, device)
        total_loss.backward()
        optimizer.step()

        mean_loss += total_loss.cpu().detach().numpy()

        # Measure accuracy

        gt = np.squeeze(sample_batched['gt'].cpu().numpy())

        label_out = torch.nn.functional.softmax(output, dim = 1)
        label_out = label_out.cpu().detach().numpy()
        label_out = np.squeeze(label_out)

        labels = np.argmax(label_out, axis=0)
        valid_mask = gt != -1
        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])
        curr_false = np.sum(valid_mask) - curr_correct
        n_correct += curr_correct
        n_false += curr_false

    mean_loss /= len(train_loader)
    train_acc = n_correct / (n_correct + n_false)

    print('Train loss: %f, train acc: %f' % (mean_loss, train_acc))


    n_correct = 0
    n_false = 0


    for i_batch, sample_batched in enumerate(val_loader):

        image = sample_batched['image'].to(device)
        image_np = np.squeeze(sample_batched['image_original'].cpu().numpy())
        gt = np.squeeze(sample_batched['gt'].cpu().numpy())


        label_out = model(image)
        label_out = torch.nn.functional.softmax(label_out, dim = 1)
        label_out = label_out.cpu().detach().numpy()
        label_out = np.squeeze(label_out)

        labels = np.argmax(label_out, axis=0)

        if plot_val:

            color_label = np.zeros((resolution[1], resolution[0], 3))

            for key, val in id_to_class.items():
                color_label[labels == key] = class_to_color[val]

            plt.figure()
            plt.imshow((image_np/255) * 0.5 + (color_label/255) * 0.5)
            plt.show()

            plt.figure()
            plt.imshow(color_label.astype(np.uint8))
            plt.show()

        valid_mask = gt != -1
        curr_correct = np.sum(gt[valid_mask] == labels[valid_mask])
        curr_false = np.sum(valid_mask) - curr_correct
        n_correct += curr_correct
        n_false += curr_false


    total_acc = n_correct / (n_correct + n_false)

    if best_val_acc < total_acc:
        best_val_acc = total_acc
        if epoch > 7:
            torch.save(model.state_dict(), pasta + 'result/' + model_name)
            print('New best validation acc. Saving...')
        best_epoch = epoch

    if (epoch - best_epoch) > patience:
        print("Finishing training, best validation acc", best_val_acc)
        break

    print('Val acc: %f -- Best val acc: %f -- epoch %d.' % (total_acc, best_val_acc, best_epoch))
print("Exec time: %s minutes ---" % str((time.time() - start_time)/60))

"""Inference Sample"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  4 19:46:01 2018

@author: caiom
"""

from google.colab import drive
drive.mount('/content/drive')

pasta = '/content/drive/MyDrive/UFSB/Projetos e orientações/Projeto Embrapa-ADAB-UFSB Viroses Mamão/Bases Dados e Experimentos/unet-ufu/'


import os.path as osp
import numpy as np
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
#import vgg_unet
import glob
import torch
import cv2

img_folder = pasta + 'Test/'
model_path = pasta + 'result/segm2.pth'
save_dir = pasta + 'result/'

#Width x Height - MUST be divisible by 32
#resolution = (800, 448)
resolution = (1600, 896)
#resolution = (2400, 1344)
#resolution = (3200, 1792)
#resolution = (4000, 2240)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Color in RGB
class_to_color = {'Solo': (127, 0, 0) , 'Saudavel': (0, 127, 127), 'Doenca': (0, 255, 0)}
class_to_id = {'Solo': 0, 'Saudavel': 1, 'Doenca': 2} #considerar a distância entre os rótulos
id_to_class = {v: k for k, v in class_to_id.items()}
nClasses = 3
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]


#model = vgg_unet.UNetVgg(nClasses)
model = UNetVgg(nClasses)
model.load_state_dict(torch.load(model_path))
model.eval()
model.to(device)

img_list = glob.glob(osp.join(img_folder, '*.JPG'))

for img_path in img_list:

        imgName = img_path.split('.')[0].split('/')[-1]
        print(imgName)

        img_np = cv2.imread(img_path, cv2.IMREAD_IGNORE_ORIENTATION + cv2.IMREAD_COLOR)
        img_np = cv2.resize(img_np, (resolution[0], resolution[1]))[..., ::-1]
        img_np = np.ascontiguousarray(img_np)

        img_pt = np.copy(img_np).astype(np.float32) / 255.0
        for i in range(3):
            img_pt[..., i] -= mean[i]
            img_pt[..., i] /= std[i]

        img_pt = img_pt.transpose(2,0,1)

        img_pt = torch.from_numpy(img_pt[None, ...]).to(device)

        label_out = model(img_pt)
        label_out = torch.nn.functional.softmax(label_out, dim = 1)
        label_out = label_out.cpu().detach().numpy()
        label_out = np.squeeze(label_out)

        labels = np.argmax(label_out, axis=0)
        #print(labels)

        color_label = np.zeros((resolution[1], resolution[0], 3))

        for key, val in id_to_class.items():
            color_label[labels == key] = class_to_color[val]

        plt.figure()
        plt.imshow((img_np/255) * 0.5 + (color_label/255) * 0.5)
        plt.savefig(save_dir + "pred_" + imgName + ".JPG")
        plt.show()

        plt.figure()
        plt.imshow(color_label.astype(np.uint8))
        plt.savefig(save_dir + "pred_" + imgName + "_GT" + ".JPG")
        plt.show()

"""Monitor de Performance"""

!pip install --upgrade wandb
import wandb
wandb.init()

"""Clean GPU"""

import gc

model = None
gc.collect()
torch.cuda.empty_cache()

image